{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1vCphml0r5-YGuwHwyqMYsVNfw29DxmCk","authorship_tag":"ABX9TyNGxD1TG6To8UyNF65wwTto"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b75c96334bf040249747036f3ac32c96":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f2b150a60c264c9eae0f873f1c0e06a5","IPY_MODEL_aa120ec01972445aa1e9786eaa7e14ef","IPY_MODEL_19c09593f3fe4579a71744f42f7d6e25"],"layout":"IPY_MODEL_e24428b919e04c8da2d0446263a1976b"}},"f2b150a60c264c9eae0f873f1c0e06a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_178814f0ebe14a15b737d212781a42cb","placeholder":"‚Äã","style":"IPY_MODEL_de75f30f79f9446d9d6652f78d40214e","value":" 67%"}},"aa120ec01972445aa1e9786eaa7e14ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_716326e7ece74b0cb9afdf0bfaa659ef","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a67f5c89a6f64b73b9d4c76a98c8b74f","value":2}},"19c09593f3fe4579a71744f42f7d6e25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3febd520ba84f0e9396a96b979e9bdc","placeholder":"‚Äã","style":"IPY_MODEL_24e4aa2dd53444508c8ad6886a25755e","value":" 2/3 [00:08&lt;00:03,  3.43s/ba]"}},"e24428b919e04c8da2d0446263a1976b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"178814f0ebe14a15b737d212781a42cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de75f30f79f9446d9d6652f78d40214e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"716326e7ece74b0cb9afdf0bfaa659ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a67f5c89a6f64b73b9d4c76a98c8b74f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a3febd520ba84f0e9396a96b979e9bdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24e4aa2dd53444508c8ad6886a25755e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5iEuEX9XnoMo"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/adl-hw2')"]},{"cell_type":"code","source":["os.chdir('./multiple-choice')\n","os.system('pip install -r requirements.txt')\n","os.system('pip install transformers')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iOoXl7iG5_LD","executionInfo":{"status":"ok","timestamp":1668012295799,"user_tz":-480,"elapsed":8428,"user":{"displayName":"deep learning","userId":"13049409950151026239"}},"outputId":"b5029f04-4aff-42bd-b293-7772e2855a8e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIcRXi5Qaf7a"},"outputs":[],"source":["import argparse\n","import json\n","import logging\n","import math\n","import os\n","import random\n","from dataclasses import dataclass\n","from itertools import chain\n","from pathlib import Path\n","from typing import Optional, Union\n","\n","import datasets\n","import torch\n","import pandas as pd\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm\n","\n","import evaluate\n","import transformers\n","from accelerate import Accelerator\n","from accelerate.logging import get_logger\n","from accelerate.utils import set_seed\n","from huggingface_hub import Repository\n","from transformers import (\n","    CONFIG_MAPPING,\n","    MODEL_MAPPING,\n","    AutoConfig,\n","    AutoModelForMultipleChoice,\n","    AutoTokenizer,\n","    PreTrainedTokenizerBase,\n","    SchedulerType,\n","    default_data_collator,\n","    get_scheduler,\n",")\n","\n","from transformers.utils import PaddingStrategy, check_min_version, get_full_repo_name, send_example_telemetry\n","from pathlib import Path\n","from datasets import Dataset\n","import datasets\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","# check_min_version(\"4.24.0.dev0\")\n","\n","logger = get_logger(__name__)\n","# You should update this to your particular problem to have better documentation of `model_type`\n","MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9y0ZZ0JNaLis"},"outputs":[],"source":["def parse_args():\n","    parser = argparse.ArgumentParser(description=\"Finetune a transformers model on a multiple choice task\")\n","    parser.add_argument(\n","        \"--model_name_or_path\",\n","        type=str,\n","        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n","        required=False,\n","        default='../cache/test-mc-no-trainer/' # bert-base-uncased\n","    )\n","    parser.add_argument(\n","        \"--dataset_name\",\n","        type=str,\n","        help=\"The name of the dataset to use (via the datasets library).\",\n","        default='swag',\n","    )\n","    parser.add_argument(\n","        \"--output_dir\", \n","        type=str, \n","        help=\"Where to store the final model.\",\n","        default='../cache/test-mc-no-trainer', \n","        )    \n","    parser.add_argument(\n","        \"--pad_to_max_length\",\n","        action=\"store_true\",\n","        help=\"If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.\",\n","    )\n","\n","    #=======================================================================\n","\n","    parser.add_argument(\n","        \"--dataset_config_name\",\n","        type=str,\n","        default=None,\n","        help=\"The configuration name of the dataset to use (via the datasets library).\",\n","    )\n","    parser.add_argument(\n","        \"--train_file\", type=str, default=None, help=\"A csv or a json file containing the training data.\"\n","    )\n","    parser.add_argument(\n","        \"--validation_file\", type=str, default=None, help=\"A csv or a json file containing the validation data.\"\n","    )\n","    parser.add_argument(\n","        \"--max_length\",\n","        type=int,\n","        default=512,\n","        help=(\n","            \"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,\"\n","            \" sequences shorter will be padded if `--pad_to_max_lengh` is passed.\"\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--config_name\",\n","        type=str,\n","        default=None,\n","        help=\"Pretrained config name or path if not the same as model_name\",\n","    )\n","    parser.add_argument(\n","        \"--tokenizer_name\",\n","        type=str,\n","        default=None,\n","        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n","    )\n","    parser.add_argument(\n","        \"--use_slow_tokenizer\",\n","        action=\"store_true\",\n","        help=\"If passed, will use a slow tokenizer (not backed by the ü§ó Tokenizers library).\",\n","    )\n","    parser.add_argument(\n","        \"--per_device_train_batch_size\",\n","        type=int,\n","        default=1,\n","        help=\"Batch size (per device) for the training dataloader.\",\n","    )\n","    parser.add_argument(\n","        \"--per_device_eval_batch_size\",\n","        type=int,\n","        default=1,\n","        help=\"Batch size (per device) for the evaluation dataloader.\",\n","    )\n","    parser.add_argument(\n","        \"--learning_rate\",\n","        type=float,\n","        default=3e-5,\n","        help=\"Initial learning rate (after the potential warmup period) to use.\",\n","    )\n","    parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"Weight decay to use.\")\n","    parser.add_argument(\"--num_train_epochs\", type=int, default=5, help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\n","        \"--max_train_steps\",\n","        type=int,\n","        default=None,\n","        help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\",\n","    )\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\",\n","        type=int,\n","        default=2,\n","        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","    parser.add_argument(\n","        \"--lr_scheduler_type\",\n","        type=SchedulerType,\n","        default=\"linear\",\n","        help=\"The scheduler type to use.\",\n","        choices=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"],\n","    )\n","    parser.add_argument(\n","        \"--num_warmup_steps\", type=int, default=0, help=\"Number of steps for the warmup in the lr scheduler.\"\n","    )\n","    \n","    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n","    parser.add_argument(\n","        \"--model_type\",\n","        type=str,\n","        default=None,\n","        help=\"Model type to use if training from scratch.\",\n","        choices=MODEL_TYPES,\n","    )\n","    parser.add_argument(\n","        \"--debug\",\n","        action=\"store_true\",\n","        help=\"Activate debug mode and run training only with a subset of data.\",\n","        default=True,\n","    )\n","    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the model to the Hub.\")\n","    parser.add_argument(\n","        \"--hub_model_id\", type=str, help=\"The name of the repository to keep in sync with the local `output_dir`.\"\n","    )\n","    parser.add_argument(\"--hub_token\", type=str, help=\"The token to use to push to the Model Hub.\")\n","    parser.add_argument(\n","        \"--checkpointing_steps\",\n","        type=str,\n","        default=None,\n","        help=\"Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.\",\n","    )\n","    parser.add_argument(\n","        \"--resume_from_checkpoint\",\n","        type=str,\n","        default=None,\n","        help=\"If the training should continue from a checkpoint folder.\",\n","    )\n","    parser.add_argument(\n","        \"--with_tracking\",\n","        action=\"store_true\",\n","        help=\"Whether to enable experiment trackers for logging.\",\n","    )\n","    parser.add_argument(\n","        \"--report_to\",\n","        type=str,\n","        default=\"all\",\n","        help=(\n","            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`,'\n","            ' `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` (default) to report to all integrations.'\n","            \"Only applicable when `--with_tracking` is passed.\"\n","        ),\n","    )\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","\n","    if args.push_to_hub:\n","        assert args.output_dir is not None, \"Need an `output_dir` to create a repo when `--push_to_hub` is passed.\"\n","\n","    return args"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OiYdkmSkjGEH"},"outputs":[],"source":["args = parse_args()\n","# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n","# information sent is the one passed as arguments along with your Python/PyTorch versions.\n","send_example_telemetry(\"run_swag_no_trainer\", args)\n","\n","# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n","# If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n","# in the environment\n","accelerator_log_kwargs = {}\n","\n","if args.with_tracking:\n","    accelerator_log_kwargs[\"log_with\"] = args.report_to\n","    accelerator_log_kwargs[\"logging_dir\"] = args.output_dir\n","\n","accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n","\n","# Make one log on every process with the configuration for debugging.\n","logging.basicConfig(\n","    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","    datefmt=\"%m/%d/%Y %H:%M:%S\",\n","    level=logging.INFO,\n",")\n","logger.info(accelerator.state, main_process_only=False)\n","if accelerator.is_local_main_process:\n","    datasets.utils.logging.set_verbosity_warning()\n","    transformers.utils.logging.set_verbosity_info()\n","else:\n","    datasets.utils.logging.set_verbosity_error()\n","    transformers.utils.logging.set_verbosity_error()\n","\n","# If passed along, set the training seed now.\n","if args.seed is not None:\n","    set_seed(args.seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZ8b0za7aYDz"},"outputs":[],"source":["@dataclass\n","class DataCollatorForMultipleChoice:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs for multiple choice received.\n","\n","    Args:\n","        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n","            The tokenizer used for encoding the data.\n","        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","\n","            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n","              if provided).\n","            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n","              acceptable input length for the model if that argument is not provided.\n","            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n","              lengths).\n","        max_length (`int`, *optional*):\n","            Maximum length of the returned list and optionally padding length (see above).\n","        pad_to_multiple_of (`int`, *optional*):\n","            If set will pad the sequence to a multiple of the provided value.\n","\n","            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n","            7.5 (Volta).\n","    \"\"\"\n","\n","    tokenizer: PreTrainedTokenizerBase\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","\n","    def __call__(self, features):\n","        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n","        id_name = 'ids'\n","\n","        if label_name in features[0].keys():\n","          labels = [feature.pop(label_name) for feature in features]\n","\n","        ids = [feature.pop(id_name) for feature in features]\n","\n","        batch_size = len(features)\n","        num_choices = len(features[0][\"input_ids\"])\n","        flattened_features = [\n","            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","        ]\n","        flattened_features = list(chain(*flattened_features))\n","\n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","\n","        # Un-flatten\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        # Add back labels\n","        # if labels:\n","        if 'labels' in locals():\n","          batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)        \n","        batch[\"ids\"] = ids\n","        return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fiZy10DPjWnL"},"outputs":[],"source":["# Handle the repository creation\n","if accelerator.is_main_process:\n","    if args.push_to_hub:\n","        if args.hub_model_id is None:\n","            repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n","        else:\n","            repo_name = args.hub_model_id\n","        repo = Repository(args.output_dir, clone_from=repo_name)\n","\n","        with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n","            if \"step_*\" not in gitignore:\n","                gitignore.write(\"step_*\\n\")\n","            if \"epoch_*\" not in gitignore:\n","                gitignore.write(\"epoch_*\\n\")\n","    elif args.output_dir is not None:\n","        os.makedirs(args.output_dir, exist_ok=True)\n","accelerator.wait_for_everyone()"]},{"cell_type":"code","source":["# load model, tokenizer and config\n","config = AutoConfig.from_pretrained(args.model_name_or_path)\n","tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n","model = AutoModelForMultipleChoice.from_pretrained(args.model_name_or_path, config=config)\n","# padding\n","padding = \"max_length\" if args.pad_to_max_length else False\n","\n","# Use the device given by the `accelerator` object.\n","device = accelerator.device\n","model.to(device)"],"metadata":{"id":"xqkpQMSQ66NJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EindRWFYjm34"},"outputs":[],"source":["# Ë®≠ÂÆöË≥áÊñôÊ¨Ñ‰Ωç\n","# column_names\n","# ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label']\n","\n","ending_names = [f\"ending{i}\" for i in range(4)]\n","context_name = \"sent1\"\n","question_id_name = \"id\"\n","question_header_name = \"sent2\"\n","label_column_name = \"label\" "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u39G2ZLLzkyK"},"outputs":[],"source":["# ËΩâÊèõË≥áÊñôÊ†ºÂºè\n","def trans_format(ori_data, data_type='train_data'):\n","  new_data_json = []\n","  for old_data in ori_data:\n","    new_data = {}\n","    paragraphs = old_data['paragraphs']\n","    \n","    new_data['id'] = old_data['id']\n","    new_data['sent1'] = old_data['question']\n","    new_data['sent2'] = ''\n","    new_data['ending0'] = content_data[paragraphs[0]]\n","    new_data['ending1'] = content_data[paragraphs[1]]\n","    new_data['ending2'] = content_data[paragraphs[2]]\n","    new_data['ending3'] = content_data[paragraphs[3]]\n","    if data_type == 'train_data':\n","      new_data['label'] = paragraphs.index(old_data['relevant'])\n","    new_data_json.append(new_data)\n","  return new_data_json\n","\n","def preprocess_function(examples):\n","\n","    question_headers = examples[question_header_name]\n","    question_ids = examples[question_id_name]\n","    first_sentences = [[context] * 4 for context in examples[context_name]]\n","    second_sentences = [\n","        [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n","    ]\n","    \n","    if label_column_name in examples:\n","      labels = examples[label_column_name]\n","\n","    # Flatten out\n","    first_sentences = list(chain(*first_sentences))\n","    second_sentences = list(chain(*second_sentences))\n","\n","    # Tokenize\n","    tokenized_examples = tokenizer(\n","        first_sentences,\n","        second_sentences,\n","        max_length=args.max_length,\n","        padding=padding,\n","        truncation=True,\n","    )\n","    # Un-flatten\n","    tokenized_inputs = {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n","    if label_column_name in examples:\n","      tokenized_inputs[\"labels\"] = labels\n","    tokenized_inputs[\"ids\"] = question_ids\n","\n","    return tokenized_inputs"]},{"cell_type":"markdown","metadata":{"id":"uqQu_Ao1b1Gq"},"source":["## load_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KuHIKlE2y7h7"},"outputs":[],"source":["# ËÆÄÂÖ•ÂéüÂßãË≥áÊñô\n","train_data = json.loads(Path('../data/train.json').read_text())\n","val_data = json.loads(Path('../data/valid.json').read_text())\n","test_data = json.loads(Path('../data/test.json').read_text())\n","content_data = json.loads(Path('../data/context.json').read_text())\n","\n","# ËΩâÊèõË≥áÊñôÊ†ºÂºè\n","new_train_data = trans_format(train_data)\n","new_val_data = trans_format(val_data)\n","new_test_data = trans_format(test_data,'test_data')\n","\n","new_train_data = Dataset.from_list(new_train_data)\n","new_val_data = Dataset.from_list(new_val_data)\n","new_test_data = Dataset.from_list(new_test_data)\n","\n","new_raw_datasets = datasets.DatasetDict({\"train\":new_train_data,\"validation\":new_val_data })\n","test_raw_datasets = datasets.DatasetDict({\"test\":new_test_data})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":65,"referenced_widgets":["b75c96334bf040249747036f3ac32c96","f2b150a60c264c9eae0f873f1c0e06a5","aa120ec01972445aa1e9786eaa7e14ef","19c09593f3fe4579a71744f42f7d6e25","e24428b919e04c8da2d0446263a1976b","178814f0ebe14a15b737d212781a42cb","de75f30f79f9446d9d6652f78d40214e","716326e7ece74b0cb9afdf0bfaa659ef","a67f5c89a6f64b73b9d4c76a98c8b74f","a3febd520ba84f0e9396a96b979e9bdc","24e4aa2dd53444508c8ad6886a25755e"]},"executionInfo":{"elapsed":8773,"status":"ok","timestamp":1668012310991,"user":{"displayName":"deep learning","userId":"13049409950151026239"},"user_tz":-480},"id":"lBa5Xt1B9RhC","outputId":"8828415b-fa6d-4474-b2ca-1b7daedff2a9"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b75c96334bf040249747036f3ac32c96"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["DataCollatorWithPadding\n"]}],"source":["# test data\n","with accelerator.main_process_first():\n","    processed_datasets_test = test_raw_datasets.map(\n","        preprocess_function, batched=True, remove_columns=test_raw_datasets[\"test\"].column_names\n","    )\n","\n","test_dataset = processed_datasets_test[\"test\"]\n","\n","\n","# DataLoaders creation:\n","if args.pad_to_max_length:\n","    print(\"args.pad_to_max_length\",args.pad_to_max_length)\n","    # If padding was already done ot max length, we use the default data collator that will just convert everything\n","    # to tensors.\n","    data_collator = default_data_collator\n","else:\n","    print(\"DataCollatorWithPadding\")\n","    # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n","    # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n","    # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n","    data_collator = DataCollatorForMultipleChoice(\n","        tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n","    )\n","\n","test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)"]},{"cell_type":"markdown","source":["# do predict"],"metadata":{"id":"10NhdVfjG3Vw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUk98iZ0XuZ7"},"outputs":[],"source":["def flat_list(l):\n","  return [item for sublist in l for item in sublist]\n","def trans_ind(row):\n","  return row['paragraphs'][row['result']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZlQr1X-rA2j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668012592938,"user_tz":-480,"elapsed":281953,"user":{"displayName":"deep learning","userId":"13049409950151026239"}},"outputId":"32e5e361-5c3a-4ff8-918b-dd6eed825e54"},"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]}],"source":["# test\n","test_dataloader = accelerator.prepare(test_dataloader)\n","# Prepare everything with our `accelerator`.\n","model = accelerator.prepare(model)\n","\n","id_list = []\n","result_list = []\n","model.eval()\n","for step, batch in enumerate(test_dataloader):\n","    with torch.no_grad():\n","        id_list.append(batch['ids'])\n","        batch.pop('ids', None)\n","        outputs = model(**batch)\n","    predictions = outputs.logits.argmax(dim=-1).tolist()\n","    result_list.append(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQ0u78RwrOlp"},"outputs":[],"source":["# Â±ïÈñãlist\n","flat_id = flat_list(id_list)\n","flat_result = flat_list(result_list)\n","test_df = pd.DataFrame({'id': flat_id,'result': flat_result})\n","test_json_df = pd.DataFrame.from_dict(test_data)\n","\n","# Âêà‰Ωµ\n","merge = test_df.merge(test_json_df[['id','paragraphs']], on='id')\n","merge['result_ind'] = merge.apply(lambda row : trans_ind(row), axis = 1)\n","merge = merge[['id','result_ind']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbpxY-vm-DYi"},"outputs":[],"source":["# ÂÑ≤Â≠òÁµêÊûú\n","pred_test_fname = f\"../data/mc.csv\"\n","merge.to_csv(pred_test_fname, encoding='utf-8', index=False)"]}]}