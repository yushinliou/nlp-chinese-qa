{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1-cETj2UIQvQ9JzFbAdJUckS4T1qxalSm","authorship_tag":"ABX9TyPi5OZQes/B8OHOfmNlEqB+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"6cdebb3e92664a749f43efe32faaea64":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7921c4ba83544c629934e649507d2d74","IPY_MODEL_0c997337e18e46eab5810280b0783d7e","IPY_MODEL_a8c757fd8a0f431bb024beb9d3385be7"],"layout":"IPY_MODEL_17b3b092babd46efafb2ea7ee687a6c6"}},"7921c4ba83544c629934e649507d2d74":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f76cc2ab3ab4eb6af33e9ebe4216a68","placeholder":"‚Äã","style":"IPY_MODEL_258bd42089e64fa9b9fd9afa7a882a59","value":"  0%"}},"0c997337e18e46eab5810280b0783d7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_535b627d7fb24c3e885b46bee389f1b6","max":1700,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e901e560243243858ad668867a36660e","value":0}},"a8c757fd8a0f431bb024beb9d3385be7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a60fdcc5b6204506bae157be0315311a","placeholder":"‚Äã","style":"IPY_MODEL_4630bfee495647f3ae15a7f147f1411e","value":" 0/1700 [00:00&lt;?, ?it/s]"}},"17b3b092babd46efafb2ea7ee687a6c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f76cc2ab3ab4eb6af33e9ebe4216a68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"258bd42089e64fa9b9fd9afa7a882a59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"535b627d7fb24c3e885b46bee389f1b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e901e560243243858ad668867a36660e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a60fdcc5b6204506bae157be0315311a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4630bfee495647f3ae15a7f147f1411e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1395b679cf384d239374474cda9a6be5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_69315ba3b55b4068b1648eaba7d5040d","IPY_MODEL_1570a0fda9814766a6a2963ad4ca597c","IPY_MODEL_4e36c505c22f42ff8c9ac180a4dac089"],"layout":"IPY_MODEL_fd43f2eeffeb4d42aef322057d1570f8"}},"69315ba3b55b4068b1648eaba7d5040d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ebce55b9def49acb3b2144551e5af96","placeholder":"‚Äã","style":"IPY_MODEL_b89625cd474f41f1827f79a20f40db1d","value":" 67%"}},"1570a0fda9814766a6a2963ad4ca597c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d8b1a3b7aae4b2590667a1e21739116","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8cc604e6a02440f7989309a1238ea860","value":2}},"4e36c505c22f42ff8c9ac180a4dac089":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3922021c64234e1ea3f45c07e60199e5","placeholder":"‚Äã","style":"IPY_MODEL_a71ffba1bf134678b729680598cbd84c","value":" 2/3 [00:03&lt;00:01,  1.61s/ba]"}},"fd43f2eeffeb4d42aef322057d1570f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ebce55b9def49acb3b2144551e5af96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b89625cd474f41f1827f79a20f40db1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d8b1a3b7aae4b2590667a1e21739116":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cc604e6a02440f7989309a1238ea860":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3922021c64234e1ea3f45c07e60199e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a71ffba1bf134678b729680598cbd84c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4015175eb084467fb162795831dee2d0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ceb453a362d404ba8b489f3f1555355","IPY_MODEL_a48cf990dd0648a4b0abcb708f30db64","IPY_MODEL_f09430542ee14272b84ceb25baad3c17"],"layout":"IPY_MODEL_b73b0428fc284250a16d1d1687b7afee"}},"9ceb453a362d404ba8b489f3f1555355":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84ff4aacadf149fba633df830a32f9eb","placeholder":"‚Äã","style":"IPY_MODEL_d712feb133ec46aba163e39ec00a0c49","value":"Downloading builder script: 100%"}},"a48cf990dd0648a4b0abcb708f30db64":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_02a2873e67cc43269c3fba5f4e7c9837","max":4203,"min":0,"orientation":"horizontal","style":"IPY_MODEL_175c71dac8f049b7a7b507f883d182ab","value":4203}},"f09430542ee14272b84ceb25baad3c17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbb78e026c45439388e7f635d76351d1","placeholder":"‚Äã","style":"IPY_MODEL_d7ed70fabdf947d9ba96f4c4cc9356bd","value":" 4.20k/4.20k [00:00&lt;00:00, 126kB/s]"}},"b73b0428fc284250a16d1d1687b7afee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84ff4aacadf149fba633df830a32f9eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d712feb133ec46aba163e39ec00a0c49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02a2873e67cc43269c3fba5f4e7c9837":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"175c71dac8f049b7a7b507f883d182ab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dbb78e026c45439388e7f635d76351d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7ed70fabdf947d9ba96f4c4cc9356bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5iEuEX9XnoMo"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/adl-hw2')"]},{"cell_type":"code","source":["os.chdir('./multiple-choice')\n","os.system('pip install -r requirements.txt')\n","os.system('pip install transformers')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iOoXl7iG5_LD","executionInfo":{"status":"ok","timestamp":1668006072731,"user_tz":-480,"elapsed":17716,"user":{"displayName":"deep learning","userId":"13049409950151026239"}},"outputId":"f29c9ac2-928a-4dfb-867e-0e4930c062aa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIcRXi5Qaf7a"},"outputs":[],"source":["import argparse\n","import json\n","import logging\n","import math\n","import os\n","import random\n","from dataclasses import dataclass\n","from itertools import chain\n","from pathlib import Path\n","from typing import Optional, Union\n","\n","import datasets\n","import torch\n","import pandas as pd\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm\n","\n","import evaluate\n","import transformers\n","from accelerate import Accelerator\n","from accelerate.logging import get_logger\n","from accelerate.utils import set_seed\n","from huggingface_hub import Repository\n","from transformers import (\n","    CONFIG_MAPPING,\n","    MODEL_MAPPING,\n","    AutoConfig,\n","    AutoModelForMultipleChoice,\n","    AutoTokenizer,\n","    PreTrainedTokenizerBase,\n","    SchedulerType,\n","    default_data_collator,\n","    get_scheduler,\n",")\n","\n","from transformers.utils import PaddingStrategy, check_min_version, get_full_repo_name, send_example_telemetry\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","# check_min_version(\"4.24.0.dev0\")\n","\n","logger = get_logger(__name__)\n","# You should update this to your particular problem to have better documentation of `model_type`\n","MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9y0ZZ0JNaLis"},"outputs":[],"source":["def parse_args():\n","    parser = argparse.ArgumentParser(description=\"Finetune a transformers model on a multiple choice task\")\n","    parser.add_argument(\n","        \"--model_name_or_path\",\n","        type=str,\n","        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n","        required=False,\n","        default='bert-base-chinese' # bert-base-uncased\n","    )\n","    parser.add_argument(\n","        \"--dataset_name\",\n","        type=str,\n","        help=\"The name of the dataset to use (via the datasets library).\",\n","        default='swag',\n","    )\n","    parser.add_argument(\n","        \"--output_dir\", \n","        type=str, \n","        help=\"Where to store the final model.\",\n","        default='../cache/test-mc-no-trainer', \n","        )    \n","    parser.add_argument(\n","        \"--pad_to_max_length\",\n","        action=\"store_true\",\n","        help=\"If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.\",\n","    )\n","\n","    #=======================================================================\n","\n","    parser.add_argument(\n","        \"--dataset_config_name\",\n","        type=str,\n","        default=None,\n","        help=\"The configuration name of the dataset to use (via the datasets library).\",\n","    )\n","    parser.add_argument(\n","        \"--train_file\", type=str, default=None, help=\"A csv or a json file containing the training data.\"\n","    )\n","    parser.add_argument(\n","        \"--validation_file\", type=str, default=None, help=\"A csv or a json file containing the validation data.\"\n","    )\n","    parser.add_argument(\n","        \"--max_length\",\n","        type=int,\n","        default=512,\n","        help=(\n","            \"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,\"\n","            \" sequences shorter will be padded if `--pad_to_max_lengh` is passed.\"\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--config_name\",\n","        type=str,\n","        default=None,\n","        help=\"Pretrained config name or path if not the same as model_name\",\n","    )\n","    parser.add_argument(\n","        \"--tokenizer_name\",\n","        type=str,\n","        default=None,\n","        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n","    )\n","    parser.add_argument(\n","        \"--use_slow_tokenizer\",\n","        action=\"store_true\",\n","        help=\"If passed, will use a slow tokenizer (not backed by the ü§ó Tokenizers library).\",\n","    )\n","    parser.add_argument(\n","        \"--per_device_train_batch_size\",\n","        type=int,\n","        default=1,\n","        help=\"Batch size (per device) for the training dataloader.\",\n","    )\n","    parser.add_argument(\n","        \"--per_device_eval_batch_size\",\n","        type=int,\n","        default=1,\n","        help=\"Batch size (per device) for the evaluation dataloader.\",\n","    )\n","    parser.add_argument(\n","        \"--learning_rate\",\n","        type=float,\n","        default=3e-5,\n","        help=\"Initial learning rate (after the potential warmup period) to use.\",\n","    )\n","    parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"Weight decay to use.\")\n","    parser.add_argument(\"--num_train_epochs\", type=int, default=5, help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\n","        \"--max_train_steps\",\n","        type=int,\n","        default=None,\n","        help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\",\n","    )\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\",\n","        type=int,\n","        default=2,\n","        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","    parser.add_argument(\n","        \"--lr_scheduler_type\",\n","        type=SchedulerType,\n","        default=\"linear\",\n","        help=\"The scheduler type to use.\",\n","        choices=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"],\n","    )\n","    parser.add_argument(\n","        \"--num_warmup_steps\", type=int, default=0, help=\"Number of steps for the warmup in the lr scheduler.\"\n","    )\n","    \n","    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n","    parser.add_argument(\n","        \"--model_type\",\n","        type=str,\n","        default=None,\n","        help=\"Model type to use if training from scratch.\",\n","        choices=MODEL_TYPES,\n","    )\n","    parser.add_argument(\n","        \"--debug\",\n","        action=\"store_true\",\n","        help=\"Activate debug mode and run training only with a subset of data.\",\n","        default=True,\n","    )\n","    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the model to the Hub.\")\n","    parser.add_argument(\n","        \"--hub_model_id\", type=str, help=\"The name of the repository to keep in sync with the local `output_dir`.\"\n","    )\n","    parser.add_argument(\"--hub_token\", type=str, help=\"The token to use to push to the Model Hub.\")\n","    parser.add_argument(\n","        \"--checkpointing_steps\",\n","        type=str,\n","        default=None,\n","        help=\"Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.\",\n","    )\n","    parser.add_argument(\n","        \"--resume_from_checkpoint\",\n","        type=str,\n","        default=None,\n","        help=\"If the training should continue from a checkpoint folder.\",\n","    )\n","    parser.add_argument(\n","        \"--with_tracking\",\n","        action=\"store_true\",\n","        help=\"Whether to enable experiment trackers for logging.\",\n","    )\n","    parser.add_argument(\n","        \"--report_to\",\n","        type=str,\n","        default=\"all\",\n","        help=(\n","            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`,'\n","            ' `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` (default) to report to all integrations.'\n","            \"Only applicable when `--with_tracking` is passed.\"\n","        ),\n","    )\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","\n","    if args.push_to_hub:\n","        assert args.output_dir is not None, \"Need an `output_dir` to create a repo when `--push_to_hub` is passed.\"\n","\n","    return args"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OiYdkmSkjGEH"},"outputs":[],"source":["args = parse_args()\n","# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n","# information sent is the one passed as arguments along with your Python/PyTorch versions.\n","send_example_telemetry(\"run_swag_no_trainer\", args)\n","\n","# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n","# If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n","# in the environment\n","accelerator_log_kwargs = {}\n","\n","if args.with_tracking:\n","    accelerator_log_kwargs[\"log_with\"] = args.report_to\n","    accelerator_log_kwargs[\"logging_dir\"] = args.output_dir\n","\n","accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n","\n","# Make one log on every process with the configuration for debugging.\n","logging.basicConfig(\n","    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","    datefmt=\"%m/%d/%Y %H:%M:%S\",\n","    level=logging.INFO,\n",")\n","logger.info(accelerator.state, main_process_only=False)\n","if accelerator.is_local_main_process:\n","    datasets.utils.logging.set_verbosity_warning()\n","    transformers.utils.logging.set_verbosity_info()\n","else:\n","    datasets.utils.logging.set_verbosity_error()\n","    transformers.utils.logging.set_verbosity_error()\n","\n","# If passed along, set the training seed now.\n","if args.seed is not None:\n","    set_seed(args.seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZ8b0za7aYDz"},"outputs":[],"source":["@dataclass\n","class DataCollatorForMultipleChoice:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs for multiple choice received.\n","\n","    Args:\n","        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n","            The tokenizer used for encoding the data.\n","        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","\n","            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n","              if provided).\n","            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n","              acceptable input length for the model if that argument is not provided.\n","            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n","              lengths).\n","        max_length (`int`, *optional*):\n","            Maximum length of the returned list and optionally padding length (see above).\n","        pad_to_multiple_of (`int`, *optional*):\n","            If set will pad the sequence to a multiple of the provided value.\n","\n","            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n","            7.5 (Volta).\n","    \"\"\"\n","\n","    tokenizer: PreTrainedTokenizerBase\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","\n","    def __call__(self, features):\n","        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n","        id_name = 'ids'\n","\n","        if label_name in features[0].keys():\n","          labels = [feature.pop(label_name) for feature in features]\n","\n","        ids = [feature.pop(id_name) for feature in features]\n","\n","        batch_size = len(features)\n","        num_choices = len(features[0][\"input_ids\"])\n","        flattened_features = [\n","            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","        ]\n","        flattened_features = list(chain(*flattened_features))\n","\n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","\n","        # Un-flatten\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        # Add back labels\n","        # if labels:\n","        if 'labels' in locals():\n","          batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)        \n","        batch[\"ids\"] = ids\n","        return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fiZy10DPjWnL"},"outputs":[],"source":["# Handle the repository creation\n","if accelerator.is_main_process:\n","    if args.push_to_hub:\n","        if args.hub_model_id is None:\n","            repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n","        else:\n","            repo_name = args.hub_model_id\n","        repo = Repository(args.output_dir, clone_from=repo_name)\n","\n","        with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n","            if \"step_*\" not in gitignore:\n","                gitignore.write(\"step_*\\n\")\n","            if \"epoch_*\" not in gitignore:\n","                gitignore.write(\"epoch_*\\n\")\n","    elif args.output_dir is not None:\n","        os.makedirs(args.output_dir, exist_ok=True)\n","accelerator.wait_for_everyone()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MGNJUJx0jpNv"},"outputs":[],"source":["# Ë®≠ÂÆöÊ®°Âûã\n","# Load pretrained model and tokenizer\n","# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","# download model & vocab.\n","if args.config_name:\n","    config = AutoConfig.from_pretrained(args.model_name_or_path)\n","elif args.model_name_or_path:\n","    config = AutoConfig.from_pretrained(args.model_name_or_path)\n","else:\n","    config = CONFIG_MAPPING[args.model_type]()\n","    logger.warning(\"You are instantiating a new config instance from scratch.\")\n","\n","# ÂàùÂßãÂåñ tokenizer\n","if args.tokenizer_name:\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n","elif args.model_name_or_path:\n","    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n","else:\n","    raise ValueError(\n","        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n","        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n","    )\n","\n","# ÂàùÂßãÂåñ model\n","if args.model_name_or_path:\n","    # È†êË®ìÁ∑¥Ê®°Âûã \n","    model = AutoModelForMultipleChoice.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","    )\n","else:\n","    # ÂæûÈ†≠Ë®ìÁ∑¥\n","    logger.info(\"Training new model from scratch\")\n","    model = AutoModelForMultipleChoice.from_config(config)\n","\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# Ë®≠ÂÆö padding\n","# Preprocessing the datasets.\n","# First we tokenize all the texts.\n","padding = \"max_length\" if args.pad_to_max_length else False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EindRWFYjm34"},"outputs":[],"source":["# Ë®≠ÂÆöË≥áÊñôÊ¨Ñ‰Ωç\n","# column_names\n","# ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label']\n","\n","ending_names = [f\"ending{i}\" for i in range(4)]\n","context_name = \"sent1\"\n","question_id_name = \"id\"\n","question_header_name = \"sent2\"\n","label_column_name = \"label\" "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u39G2ZLLzkyK"},"outputs":[],"source":["# ËΩâÊèõË≥áÊñôÊ†ºÂºè\n","def trans_format(ori_data, data_type='train_data'):\n","  new_data_json = []\n","  for old_data in ori_data:\n","    new_data = {}\n","    paragraphs = old_data['paragraphs']\n","    \n","    new_data['id'] = old_data['id']\n","    new_data['sent1'] = old_data['question']\n","    new_data['sent2'] = ''\n","    new_data['ending0'] = content_data[paragraphs[0]]\n","    new_data['ending1'] = content_data[paragraphs[1]]\n","    new_data['ending2'] = content_data[paragraphs[2]]\n","    new_data['ending3'] = content_data[paragraphs[3]]\n","    if data_type == 'train_data':\n","      new_data['label'] = paragraphs.index(old_data['relevant'])\n","    new_data_json.append(new_data)\n","  return new_data_json\n","\n","def preprocess_function(examples):\n","\n","    question_headers = examples[question_header_name]\n","    question_ids = examples[question_id_name]\n","    first_sentences = [[context] * 4 for context in examples[context_name]]\n","    second_sentences = [\n","        [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n","    ]\n","    \n","    if label_column_name in examples:\n","      labels = examples[label_column_name]\n","\n","    # Flatten out\n","    first_sentences = list(chain(*first_sentences))\n","    second_sentences = list(chain(*second_sentences))\n","\n","    # Tokenize\n","    tokenized_examples = tokenizer(\n","        first_sentences,\n","        second_sentences,\n","        max_length=args.max_length,\n","        padding=padding,\n","        truncation=True,\n","    )\n","    # Un-flatten\n","    tokenized_inputs = {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n","    if label_column_name in examples:\n","      tokenized_inputs[\"labels\"] = labels\n","    tokenized_inputs[\"ids\"] = question_ids\n","\n","    return tokenized_inputs"]},{"cell_type":"markdown","metadata":{"id":"uqQu_Ao1b1Gq"},"source":["## load_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KuHIKlE2y7h7"},"outputs":[],"source":["from pathlib import Path\n","from datasets import Dataset\n","import datasets\n","\n","# ËÆÄÂÖ•ÂéüÂßãË≥áÊñô\n","train_data = json.loads(Path('../data/train.json').read_text())\n","val_data = json.loads(Path('../data/valid.json').read_text())\n","test_data = json.loads(Path('../data/test.json').read_text())\n","content_data = json.loads(Path('../data/context.json').read_text())\n","\n","# ËΩâÊèõË≥áÊñôÊ†ºÂºè\n","new_train_data = trans_format(train_data)\n","new_val_data = trans_format(val_data)\n","new_test_data = trans_format(test_data,'test_data')\n","\n","new_train_data = Dataset.from_list(new_train_data)\n","new_val_data = Dataset.from_list(new_val_data)\n","new_test_data = Dataset.from_list(new_test_data)\n","\n","new_raw_datasets = datasets.DatasetDict({\"train\":new_train_data,\"validation\":new_val_data })\n","test_raw_datasets = datasets.DatasetDict({\"test\":new_test_data})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CU2MJ4LTY7QM"},"outputs":[],"source":["# train data\n","with accelerator.main_process_first():\n","    processed_datasets = new_raw_datasets.map(\n","        preprocess_function, batched=True, remove_columns=new_raw_datasets[\"train\"].column_names\n","    )\n","\n","train_dataset = processed_datasets[\"train\"]\n","eval_dataset = processed_datasets[\"validation\"]\n","\n","# Log a few random samples from the training set:\n","for index in random.sample(range(len(train_dataset)), 3):\n","    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["1395b679cf384d239374474cda9a6be5","69315ba3b55b4068b1648eaba7d5040d","1570a0fda9814766a6a2963ad4ca597c","4e36c505c22f42ff8c9ac180a4dac089","fd43f2eeffeb4d42aef322057d1570f8","4ebce55b9def49acb3b2144551e5af96","b89625cd474f41f1827f79a20f40db1d","4d8b1a3b7aae4b2590667a1e21739116","8cc604e6a02440f7989309a1238ea860","3922021c64234e1ea3f45c07e60199e5","a71ffba1bf134678b729680598cbd84c"]},"executionInfo":{"elapsed":3353,"status":"ok","timestamp":1667914026201,"user":{"displayName":"deep learning","userId":"13049409950151026239"},"user_tz":-480},"id":"lBa5Xt1B9RhC","outputId":"0e9571d7-c82b-46f6-da82-65d006329210"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1395b679cf384d239374474cda9a6be5"}},"metadata":{}}],"source":["# test data\n","with accelerator.main_process_first():\n","    processed_datasets_test = test_raw_datasets.map(\n","        preprocess_function, batched=True, remove_columns=test_raw_datasets[\"test\"].column_names\n","    )\n","\n","test_dataset = processed_datasets_test[\"test\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1667914026202,"user":{"displayName":"deep learning","userId":"13049409950151026239"},"user_tz":-480},"id":"1f8bEe8fkD4d","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8cd18a1b-d486-4d01-fcc4-7279e9a9d434"},"outputs":[{"output_type":"stream","name":"stdout","text":["DataCollatorWithPadding\n"]}],"source":["# DataLoaders creation:\n","if args.pad_to_max_length:\n","    print(\"args.pad_to_max_length\",args.pad_to_max_length)\n","    # If padding was already done ot max length, we use the default data collator that will just convert everything\n","    # to tensors.\n","    data_collator = default_data_collator\n","else:\n","    print(\"DataCollatorWithPadding\")\n","    # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n","    # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n","    # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n","    data_collator = DataCollatorForMultipleChoice(\n","        tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n","    )\n","\n","train_dataloader = DataLoader(\n","    train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",")\n","eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n","test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0q_d1PLBkS90"},"outputs":[],"source":["# Optimizer\n","# Split weights in two groups, one with weight decay and the other not.\n","no_decay = [\"bias\", \"LayerNorm.weight\"]\n","optimizer_grouped_parameters = [\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","        \"weight_decay\": args.weight_decay,\n","    },\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","        \"weight_decay\": 0.0,\n","    },\n","]\n","optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n","\n","# Use the device given by the `accelerator` object.\n","device = accelerator.device\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Vm4M7J9kYpi"},"outputs":[],"source":["# Scheduler and math around the number of training steps.\n","overrode_max_train_steps = False\n","num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n","if args.max_train_steps is None:\n","    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n","    overrode_max_train_steps = True\n","\n","lr_scheduler = get_scheduler(\n","    name=args.lr_scheduler_type,\n","    optimizer=optimizer,\n","    num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps,\n","    num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",")\n","\n","# Prepare everything with our `accelerator`.\n","model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n","    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["4015175eb084467fb162795831dee2d0","9ceb453a362d404ba8b489f3f1555355","a48cf990dd0648a4b0abcb708f30db64","f09430542ee14272b84ceb25baad3c17","b73b0428fc284250a16d1d1687b7afee","84ff4aacadf149fba633df830a32f9eb","d712feb133ec46aba163e39ec00a0c49","02a2873e67cc43269c3fba5f4e7c9837","175c71dac8f049b7a7b507f883d182ab","dbb78e026c45439388e7f635d76351d1","d7ed70fabdf947d9ba96f4c4cc9356bd"]},"id":"1NK7uLpPkbyD","outputId":"6e0d0cac-4397-42fa-c777-e60181fb9b68","executionInfo":{"status":"ok","timestamp":1667914026836,"user_tz":-480,"elapsed":641,"user":{"displayName":"deep learning","userId":"13049409950151026239"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4015175eb084467fb162795831dee2d0"}},"metadata":{}}],"source":["# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n","num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n","if overrode_max_train_steps:\n","    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n","# Afterwards we recalculate our number of training epochs\n","args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n","\n","# Figure out how many steps we should save the Accelerator states\n","checkpointing_steps = args.checkpointing_steps\n","if checkpointing_steps is not None and checkpointing_steps.isdigit():\n","    checkpointing_steps = int(checkpointing_steps)\n","\n","# We need to initialize the trackers we use, and also store our configuration.\n","# The trackers initializes automatically on the main process.\n","if args.with_tracking:\n","    experiment_config = vars(args)\n","    # TensorBoard cannot log Enums, need the raw value\n","    experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n","    accelerator.init_trackers(\"swag_no_trainer\", experiment_config)\n","\n","# Metrics\n","metric = evaluate.load(\"accuracy\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["6cdebb3e92664a749f43efe32faaea64","7921c4ba83544c629934e649507d2d74","0c997337e18e46eab5810280b0783d7e","a8c757fd8a0f431bb024beb9d3385be7","17b3b092babd46efafb2ea7ee687a6c6","8f76cc2ab3ab4eb6af33e9ebe4216a68","258bd42089e64fa9b9fd9afa7a882a59","535b627d7fb24c3e885b46bee389f1b6","e901e560243243858ad668867a36660e","a60fdcc5b6204506bae157be0315311a","4630bfee495647f3ae15a7f147f1411e"]},"id":"H-OnVLJKkfOw","outputId":"f164a664-696f-46d9-b63c-116f55baf17e","executionInfo":{"status":"ok","timestamp":1667830403236,"user_tz":-480,"elapsed":15,"user":{"displayName":"deep learning","userId":"13049409950151026239"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1700 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cdebb3e92664a749f43efe32faaea64"}},"metadata":{}}],"source":["# Train!\n","total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n","\n","logger.info(\"***** Running training *****\")\n","logger.info(f\"  Num examples = {len(train_dataset)}\")\n","logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n","logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n","logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n","logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n","logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n","# Only show the progress bar once on each machine.\n","progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n","completed_steps = 0\n","starting_epoch = 0\n","\n","# Potentially load in the weights and states from a previous save\n","if args.resume_from_checkpoint:\n","    if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n","        accelerator.print(f\"Resumed from checkpoint: {args.resume_from_checkpoint}\")\n","        accelerator.load_state(args.resume_from_checkpoint)\n","        path = os.path.basename(args.resume_from_checkpoint)\n","    else:\n","        # Get the most recent checkpoint\n","        dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n","        dirs.sort(key=os.path.getctime)\n","        path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n","    # Extract `epoch_{i}` or `step_{i}`\n","    training_difference = os.path.splitext(path)[0]\n","\n","    if \"epoch\" in training_difference:\n","        starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n","        resume_step = None\n","    else:\n","        resume_step = int(training_difference.replace(\"step_\", \"\"))\n","        starting_epoch = resume_step // len(train_dataloader)\n","        resume_step -= starting_epoch * len(train_dataloader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RW3-lzz-kju2"},"outputs":[],"source":["# for epoch in range(starting_epoch, args.num_train_epochs):\n","for epoch in range(starting_epoch, 1):\n","    model.train()\n","    if args.with_tracking:\n","        total_loss = 0\n","    for step, batch in enumerate(train_dataloader):\n","        # We need to skip steps until we reach the resumed step\n","        if args.resume_from_checkpoint and epoch == starting_epoch:\n","            if resume_step is not None and step < resume_step:\n","                completed_steps += 1\n","                continue\n","\n","        with accelerator.accumulate(model):\n","            batch.pop('ids', None)\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            # We keep track of the loss at each epoch\n","            if args.with_tracking:\n","                total_loss += loss.detach().float()\n","            accelerator.backward(loss)\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","\n","        # Checks if the accelerator has performed an optimization step behind the scenes\n","        if accelerator.sync_gradients:\n","            progress_bar.update(1)\n","            completed_steps += 1\n","\n","        if isinstance(checkpointing_steps, int):\n","            if completed_steps % checkpointing_steps == 0:\n","                output_dir = f\"step_{completed_steps }\"\n","                if args.output_dir is not None:\n","                    output_dir = os.path.join(args.output_dir, output_dir)\n","                accelerator.save_state(output_dir)\n","                print(f\"save accelerator to {output_dir}\")\n","\n","        if completed_steps >= args.max_train_steps:\n","            break\n","\n","    # model validation\n","    model.eval()\n","    for step, batch in enumerate(eval_dataloader):\n","        with torch.no_grad():\n","            batch.pop('ids', None)\n","            outputs = model(**batch)\n","        predictions = outputs.logits.argmax(dim=-1)\n","        predictions, references = accelerator.gather_for_metrics((predictions, batch[\"labels\"]))\n","        metric.add_batch(\n","            predictions=predictions,\n","            references=references,\n","        )\n","\n","    eval_metric = metric.compute()\n","    accelerator.print(f\"epoch {epoch}: {eval_metric}\")\n","\n","    if args.with_tracking:\n","        accelerator.log(\n","            {\n","                \"accuracy\": eval_metric,\n","                \"train_loss\": total_loss.item() / len(train_dataloader),\n","                \"epoch\": epoch,\n","                \"step\": completed_steps,\n","            },\n","            step=completed_steps,\n","        )\n","\n","    if args.push_to_hub and epoch < args.num_train_epochs - 1:\n","        accelerator.wait_for_everyone()\n","        unwrapped_model = accelerator.unwrap_model(model)\n","        unwrapped_model.save_pretrained(\n","            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n","        )\n","        if accelerator.is_main_process:\n","            tokenizer.save_pretrained(args.output_dir)\n","            repo.push_to_hub(\n","                commit_message=f\"Training in progress epoch {epoch}\", blocking=False, auto_lfs_prune=True\n","            )\n","\n","    if args.checkpointing_steps == \"epoch\":\n","        output_dir = f\"epoch_{epoch}\"\n","        if args.output_dir is not None:\n","            output_dir = os.path.join(args.output_dir, output_dir)\n","        accelerator.save_state(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptzJaI4HWLID"},"outputs":[],"source":["# model validation\n","model.eval()\n","for step, batch in enumerate(eval_dataloader):\n","    with torch.no_grad():\n","        batch.pop('ids', None)\n","        outputs = model(**batch)\n","    predictions = outputs.logits.argmax(dim=-1)\n","    predictions, references = accelerator.gather_for_metrics((predictions, batch[\"labels\"]))\n","    metric.add_batch(\n","        predictions=predictions,\n","        references=references,\n","    )\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUk98iZ0XuZ7"},"outputs":[],"source":["def flat_list(l):\n","  return [item for sublist in l for item in sublist]\n","def trans_ind(row):\n","  return row['paragraphs'][row['result']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZlQr1X-rA2j"},"outputs":[],"source":["# test\n","test_dataloader = accelerator.prepare(test_dataloader)\n","id_list = []\n","result_list = []\n","model.eval()\n","for step, batch in enumerate(test_dataloader):\n","    with torch.no_grad():\n","        id_list.append(batch['ids'])\n","        batch.pop('ids', None)\n","        outputs = model(**batch)\n","    predictions = outputs.logits.argmax(dim=-1).tolist()\n","    result_list.append(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQ0u78RwrOlp"},"outputs":[],"source":["# Â±ïÈñãlist\n","flat_id = flat_list(id_list)\n","flat_result = flat_list(result_list)\n","test_df = pd.DataFrame({'id': flat_id,'result': flat_result})\n","test_json_df = pd.DataFrame.from_dict(test_data)\n","\n","# Âêà‰Ωµ\n","merge = test_df.merge(test_json_df[['id','paragraphs']], on='id')\n","merge['result_ind'] = merge.apply(lambda row : trans_ind(row), axis = 1)\n","merge = merge[['id','result_ind']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbpxY-vm-DYi"},"outputs":[],"source":["# ÂÑ≤Â≠òÁµêÊûú\n","pred_test_fname = f\"../data/mc.csv\"\n","merge.to_csv(pred_test_fname, encoding='utf-8', index=False)"]},{"cell_type":"code","source":["# save tokenizer\n","tokenizer.save_pretrained(args.output_dir)"],"metadata":{"id":"LHEIJIwww_Rp","executionInfo":{"status":"ok","timestamp":1667894987604,"user_tz":-480,"elapsed":628,"user":{"displayName":"deep learning","userId":"13049409950151026239"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d0a8eab4-5bea-40c3-94c2-1f94dc40bca9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bzwDoV-aiQx"},"outputs":[],"source":["if args.with_tracking:\n","    accelerator.end_training()\n","\n","if args.output_dir is not None:\n","    accelerator.wait_for_everyone()\n","    unwrapped_model = accelerator.unwrap_model(model)\n","    unwrapped_model.save_pretrained(\n","        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n","    )\n","    if accelerator.is_main_process:\n","        tokenizer.save_pretrained(args.output_dir)\n","        if args.push_to_hub:\n","            repo.push_to_hub(commit_message=\"End of training\", auto_lfs_prune=True)\n","    with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n","        json.dump({\"eval_accuracy\": eval_metric[\"accuracy\"]}, f)"]}]}